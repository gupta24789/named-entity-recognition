{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gupta24789/named-entity-recognition/blob/main/ner_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjmBuoH4PKkE"
      },
      "outputs": [],
      "source": [
        "# !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip -d embeddings/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-MrxIX-PKkH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q4kcDZdPKkI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "from torch import optim\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from seqeval import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oNY_dZFPKkJ"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX7-c3DXPKkL"
      },
      "outputs": [],
      "source": [
        "# Path('data/train').mkdir(parents = True, exist_ok= True)\n",
        "# Path('data/val').mkdir(parents = True, exist_ok= True)\n",
        "# Path('data/test').mkdir(parents = True, exist_ok= True)\n",
        "\n",
        "# os.system(\"cd data/train && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/train/sentences.txt\")\n",
        "# os.system(\"cd data/train && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/train/labels.txt\")\n",
        "# os.system(\"cd data/val && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/val/sentences.txt\")\n",
        "# os.system(\"cd data/val && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/val/labels.txt\")\n",
        "# os.system(\"cd data/test && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/test/sentences.txt\")\n",
        "# os.system(\"cd data/test && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/test/labels.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paJSBWa4PKkM"
      },
      "source": [
        "## Set Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCw9i2TgPKkM",
        "outputId": "b58da63c-d828-439a-aa7f-ca25e8a4f518"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 121\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "121"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seed = 121\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "pl.seed_everything(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6EN8jJ9PKkO"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV-soDrYPKkP"
      },
      "outputs": [],
      "source": [
        "## train\n",
        "train_sents = open(\"data/train/sentences.txt\",\"r\").readlines()\n",
        "train_tags = open(\"data/train/labels.txt\",\"r\").readlines()\n",
        "## val\n",
        "val_sents = open(\"data/val/sentences.txt\",\"r\").readlines()\n",
        "val_tags = open(\"data/val/labels.txt\",\"r\").readlines()\n",
        "## test\n",
        "test_sents = open(\"data/test/sentences.txt\",\"r\").readlines()\n",
        "test_tags = open(\"data/test/labels.txt\",\"r\").readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3rtZ9P4PKkP",
        "outputId": "f5490bfd-a93f-45e3-f264-305d2d353d6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\\n',\n",
              " 'Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"\\n']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_sents[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tM6p7azPKkQ",
        "outputId": "5a1d4d3a-79b2-46a9-d6f1-6b5595f9060e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\\n',\n",
              " 'O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O\\n']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_tags[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnOPnn0RPKkR"
      },
      "outputs": [],
      "source": [
        "X_train = [sent.strip().split(\" \") for sent in train_sents]\n",
        "y_train = [tag.strip().split(\" \") for tag in train_tags]\n",
        "\n",
        "X_val = [sent.strip().split(\" \") for sent in val_sents]\n",
        "y_val = [tag.strip().split(\" \") for tag in val_tags]\n",
        "\n",
        "X_test = [sent.strip().split(\" \") for sent in test_sents]\n",
        "y_test = [tag.strip().split(\" \") for tag in test_tags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiX6BPg1PKkR",
        "outputId": "1999631b-70a4-4256-89e3-ce5b0947daa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London',\n",
            "  'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the',\n",
            "  'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.'],\n",
            " ['Families', 'of', 'soldiers', 'killed', 'in', 'the', 'conflict', 'joined',\n",
            "  'the', 'protesters', 'who', 'carried', 'banners', 'with', 'such', 'slogans',\n",
            "  'as', '\"', 'Bush', 'Number', 'One', 'Terrorist', '\"', 'and', '\"', 'Stop',\n",
            "  'the', 'Bombings', '.', '\"']]\n"
          ]
        }
      ],
      "source": [
        "pprint((X_train[:2]), compact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5t24XqmPKkS",
        "outputId": "e9e49cc0-0e9c-4bca-dc8c-ed10292f9168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O',\n",
            "  'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']]\n"
          ]
        }
      ],
      "source": [
        "pprint(y_train[:1], compact=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyGu5mLGPKkT"
      },
      "source": [
        "## Create Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjsXxOtbPKkT",
        "outputId": "e0aa6a8a-dd42-4069-c52a-0b1cfd88a4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size : 35180\n",
            "tags : 18\n",
            "{'B-art': 2,\n",
            " 'B-eve': 8,\n",
            " 'B-geo': 7,\n",
            " 'B-gpe': 17,\n",
            " 'B-nat': 16,\n",
            " 'B-org': 12,\n",
            " 'B-per': 3,\n",
            " 'B-tim': 14,\n",
            " 'I-art': 1,\n",
            " 'I-eve': 9,\n",
            " 'I-geo': 15,\n",
            " 'I-gpe': 6,\n",
            " 'I-nat': 10,\n",
            " 'I-org': 5,\n",
            " 'I-per': 4,\n",
            " 'I-tim': 11,\n",
            " 'O': 13,\n",
            " '__PAD__': 0}\n",
            "PAD ID : 0\n"
          ]
        }
      ],
      "source": [
        "special_words = ['__PAD__','__UNK__']\n",
        "vocab = list(set(itertools.chain.from_iterable(X_train + X_val + X_test)))\n",
        "vocab = special_words + vocab\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "## TAGS\n",
        "tags = list(set(itertools.chain.from_iterable(y_train)))\n",
        "tags = ['__PAD__'] + tags\n",
        "tag2idx = {w:i for i,w in enumerate(tags)}\n",
        "idx2tag = {i:w for w,i in tag2idx.items()}\n",
        "\n",
        "\n",
        "print(f\"vocab size : {len(vocab)}\")\n",
        "print(f\"tags : {len(tag2idx)}\")\n",
        "pprint(tag2idx, compact=True)\n",
        "\n",
        "PAD_ID = word2idx['__PAD__']\n",
        "UNK_ID = word2idx['__UNK__']\n",
        "\n",
        "print(f\"PAD ID : {PAD_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc1REXSRPKkU"
      },
      "source": [
        "## Encode sent & tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_bDqyPAPKkU"
      },
      "outputs": [],
      "source": [
        "def to_sent_number(sent_list):\n",
        "    encoded = []\n",
        "    for w in sent_list:\n",
        "        encoded.append(word2idx.get(w, UNK_ID))\n",
        "    return encoded\n",
        "\n",
        "\n",
        "def to_tag_number(tag_list):\n",
        "    encoded = []\n",
        "    for tag in tag_list:\n",
        "        encoded.append(tag2idx[tag])\n",
        "    return encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_lEVY7QPKkU"
      },
      "outputs": [],
      "source": [
        "X_train_encoded = [to_sent_number(sent) for sent in X_train]\n",
        "y_train_encoded = [to_tag_number(tags) for tags in y_train]\n",
        "\n",
        "X_val_encoded = [to_sent_number(sent) for sent in X_val]\n",
        "y_val_encoded = [to_tag_number(tags) for tags in y_val]\n",
        "\n",
        "X_test_encoded = [to_sent_number(sent) for sent in X_test]\n",
        "y_test_encoded = [to_tag_number(tags) for tags in y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF2zkAoPPKkV",
        "outputId": "ba4a8d47-b64b-4ee3-b3ca-b66e2d65f960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[7138, 23213, 31249, 28708, 19796, 21565, 6163, 24430, 33070, 16029, 19865,\n",
            "  17668, 13723, 32161, 26917, 16029, 8684, 23213, 15795, 13467, 20324, 13974,\n",
            "  6309, 17678],\n",
            " [11001, 23213, 4125, 12289, 17668, 16029, 20597, 10298, 16029, 29374, 7042,\n",
            "  13520, 33280, 28632, 10046, 10710, 28399, 7218, 28474, 15327, 18029, 5774,\n",
            "  7218, 32161, 7218, 31619, 16029, 33168, 17678, 7218]]\n"
          ]
        }
      ],
      "source": [
        "pprint(X_train_encoded[:2], compact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJn7MqAmPKkV",
        "outputId": "abccf0e2-9be7-44a4-f36e-341db90a4dc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[13, 13, 13, 13, 13, 13, 7, 13, 13, 13, 13, 13, 7, 13, 13, 13, 13, 13, 17, 13,\n",
            "  13, 13, 13, 13],\n",
            " [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 3, 13,\n",
            "  13, 13, 13, 13, 13, 13, 13, 13, 13, 13]]\n"
          ]
        }
      ],
      "source": [
        "pprint(y_train_encoded[:2], compact=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFATcVmGPKkW"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5mxYHf_PKkW"
      },
      "outputs": [],
      "source": [
        "def custom_collate(batch):\n",
        "    sent = [torch.tensor(item[0]) for item in batch]\n",
        "    tag = [torch.tensor(item[1]) for item in batch]\n",
        "    lengths = torch.tensor([len(item[0]) for item in batch])\n",
        "\n",
        "    padded_sent = nn.utils.rnn.pad_sequence(sent, batch_first=True, padding_value=PAD_ID)\n",
        "    padded_tag = nn.utils.rnn.pad_sequence(tag, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    batch = {\"sent\": padded_sent, \"tag\": padded_tag, \"lengths\": lengths}\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPi-ZB_0PKkW"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(list(zip(X_train_encoded,y_train_encoded)), batch_size = 2, shuffle = False, collate_fn = custom_collate )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynJ8ukrWPKkX",
        "outputId": "62173e5e-5593-4dac-b898-9418696a9ce5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([2, 30]), torch.Size([2, 30]), torch.Size([2]))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = next(iter(train_dl))\n",
        "example['sent'].shape, example['tag'].shape,  example['lengths'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcpEOmVsPKkX",
        "outputId": "77715885-dd18-4eb2-d9e2-87b5299274e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 7138, 23213, 31249, 28708, 19796, 21565,  6163, 24430, 33070, 16029,\n",
              "         19865, 17668, 13723, 32161, 26917, 16029,  8684, 23213, 15795, 13467,\n",
              "         20324, 13974,  6309, 17678,     0,     0,     0,     0,     0,     0],\n",
              "        [11001, 23213,  4125, 12289, 17668, 16029, 20597, 10298, 16029, 29374,\n",
              "          7042, 13520, 33280, 28632, 10046, 10710, 28399,  7218, 28474, 15327,\n",
              "         18029,  5774,  7218, 32161,  7218, 31619, 16029, 33168, 17678,  7218]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example['sent']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls65yMU7PKkY",
        "outputId": "2af74277-fbbb-4137-bc94-4de60f174e9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[13, 13, 13, 13, 13, 13,  7, 13, 13, 13, 13, 13,  7, 13, 13, 13, 13, 13,\n",
              "         17, 13, 13, 13, 13, 13,  0,  0,  0,  0,  0,  0],\n",
              "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
              "          3, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example['tag']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl2HkFuCPKkY",
        "outputId": "bcefaa11-ee31-454a-837e-8b7a1b6b9b86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([24, 30])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example['lengths']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aIu3i0cPKkZ"
      },
      "outputs": [],
      "source": [
        "## dataloaders\n",
        "batch_size = 32\n",
        "train_dl = DataLoader(list(zip(X_train_encoded,y_train_encoded)), batch_size = batch_size, shuffle = True, collate_fn = custom_collate )\n",
        "val_dl = DataLoader(list(zip(X_val_encoded,y_val_encoded)), batch_size = batch_size, shuffle = False, collate_fn = custom_collate )\n",
        "test_dl = DataLoader(list(zip(X_test_encoded,y_test_encoded)), batch_size = batch_size, shuffle = False, collate_fn = custom_collate )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cODp9AwNPKkZ",
        "outputId": "3af0ea27-501d-4d3f-b0d9-ab33f6cfb02c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_15559/624670142.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  return torch.tensor(df_list)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([35180, 100])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Pretrained Vectors\n",
        "def load_pretrain_emb(filepath):\n",
        "    lines = open(filepath,\"r\").readlines()\n",
        "    embedd_dict = {}\n",
        "    for line in lines:\n",
        "        if len(line)>0:\n",
        "            tokens = line.strip().split(\" \")\n",
        "            word = tokens[0]\n",
        "            vec = tokens[1:]\n",
        "            vec = np.array(vec).astype(float)\n",
        "            embedd_dict[word]= vec\n",
        "\n",
        "    return embedd_dict\n",
        "\n",
        "def build_pretrain_embedding(filepath, vocab, emb_dim):\n",
        "    embedd_dict = load_pretrain_emb(filepath)\n",
        "\n",
        "    df_list = []\n",
        "\n",
        "    for w,i in vocab.items():\n",
        "        if w in embedd_dict:\n",
        "            df_list.append(torch.tensor(embedd_dict[w]))\n",
        "        elif w.lower() in embedd_dict:\n",
        "            df_list.append(embedd_dict[w.lower()])\n",
        "        else:\n",
        "            random_vec = np.random.normal(size = (emb_dim))\n",
        "            df_list.append(random_vec)\n",
        "\n",
        "\n",
        "    return torch.tensor(df_list)\n",
        "\n",
        "\n",
        "\n",
        "weights = build_pretrain_embedding(\"embeddings/glove.6B.100d.txt\", word2idx, emb_dim=100)\n",
        "weights.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S09Cyt34PKka"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUD3Mp7sPKka"
      },
      "outputs": [],
      "source": [
        "class NERModel(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    if you will you NLLLoss then you have to use log_softmax in forward else use CrossEntropy\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_tags, learning_rate, dropout, bidirectional = False, n_layers = 1, use_pretrained = False):\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # metrics\n",
        "        self.train_f1 = []\n",
        "        self.val_f1 = []\n",
        "        self.val_loss = []\n",
        "        self.test_f1 =[]\n",
        "        self.test_precision = []\n",
        "        self.test_recall = []\n",
        "\n",
        "        ## define loss\n",
        "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "\n",
        "\n",
        "        ## layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim= emb_dim, padding_idx= PAD_ID)\n",
        "        if use_pretrained:\n",
        "            self.embedding.weight.data.copy_(weights)\n",
        "        else:\n",
        "            self.embedding.weight.data.copy_(torch.from_numpy(self.random_embedding(vocab_size, emb_dim)))\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=bidirectional, dropout = dropout, num_layers = n_layers)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, n_tags)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def random_embedding(self, vocab_size, embedding_dim):\n",
        "        pretrain_emb = np.empty([vocab_size, embedding_dim])\n",
        "        scale = np.sqrt(3.0 / embedding_dim)\n",
        "        for index in range(1, vocab_size):\n",
        "            pretrain_emb[index, :] = np.random.uniform(-scale, scale, [1, embedding_dim])\n",
        "        return pretrain_emb\n",
        "\n",
        "    def forward(self, sent, lengths, verbose = False):\n",
        "\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(sent, lengths.to('cpu'), batch_first = True, enforce_sorted = False)\n",
        "        x, xlengths = nn.utils.rnn.pad_packed_sequence(packed_input, batch_first = True)\n",
        "\n",
        "        ## layers\n",
        "        embedded = self.embedding(x)\n",
        "        # embedded : [batch size, seq_len, emb dim]\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        # output : [batch size,seq_len,  2*hidden dim]\n",
        "        # hidden : [bidirectional * num layers, batch size, hidden dim]  ### if bidirectional == True then multiply by 2 else 1\n",
        "        # cell : [bidirectional * num layers, batch size, hidden dim]\n",
        "\n",
        "\n",
        "        output = self.dropout(output)\n",
        "        logits = self.hidden2tag(output)\n",
        "         # logits : [ batch size, seq_len, num_class]\n",
        "        logits = logits.permute(0,2,1)\n",
        "        # logits :[batch size, num class, seq len]\n",
        "        # logits = F.log_softmax(logits, dim=1)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Sent : {sent.shape}\")\n",
        "            print(f'length : {lengths.shape}')\n",
        "            print(f'x : {x.shape}')\n",
        "            print(f'xlengths : {xlengths.shape}')\n",
        "            print(f'embedded : {embedded.shape}')\n",
        "            print(f'output : {output.shape}')\n",
        "            print(f'hidden : {hidden.shape}')\n",
        "            print(f'cell : {cell.shape}')\n",
        "            print(f'logits : {logits.shape}')\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def calculate_metrics(self, y_true, y_pred, mask):\n",
        "\n",
        "        y_true = y_true  * mask\n",
        "        y_pred = y_pred * mask\n",
        "\n",
        "        ## metrics\n",
        "        y_true = y_true.cpu().numpy().tolist()\n",
        "        y_pred = y_pred.cpu().numpy().tolist()\n",
        "        y_true_label = [[idx2tag[tag] for tag in sent_tag] for sent_tag in y_true]\n",
        "        y_pred_label = [[idx2tag[tag] for tag in sent_tag] for sent_tag in y_pred]\n",
        "\n",
        "        f1_score = metrics.f1_score(y_true_label, y_pred_label)\n",
        "        precision = metrics.precision_score(y_true_label, y_pred_label)\n",
        "        recall = metrics.recall_score(y_true_label, y_pred_label)\n",
        "        return f1_score, precision, recall\n",
        "\n",
        "    def _shared_step(self, batch):\n",
        "        sents, tags, lengths = batch['sent'], batch['tag'], batch['lengths']\n",
        "        mask = (tags != PAD_ID)\n",
        "         # mask = (y_true != PAD_ID) * (y_true != tag2idx['O'])\n",
        "        logits = self(sents, lengths)\n",
        "\n",
        "        loss = self.loss_fn(logits, tags)\n",
        "        _ , preds = torch.max(logits, dim = 1)\n",
        "\n",
        "        ## calculate metrics\n",
        "        f1_score, precision, recall = self.calculate_metrics(preds, tags, mask)\n",
        "        return loss, f1_score, precision, recall\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        loss, f1_score, precision, recall = self._shared_step(batch)\n",
        "        self.train_f1.append(f1_score)\n",
        "        self.log_dict({\"train_loss\": loss, \"train_f1\": np.mean(self.train_f1)}, on_step = False, on_epoch = True, prog_bar=  True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        loss, f1_score, precision, recall = self._shared_step(batch)\n",
        "        self.val_f1.append(f1_score)\n",
        "        self.val_loss.append(loss.cpu().item())\n",
        "        self.log_dict({\"val_loss\": loss, \"val_f1\": np.mean(self.val_f1)}, on_step = False, on_epoch = True, prog_bar=  True)\n",
        "        return loss\n",
        "\n",
        "    def on_training_epoch_end(self):\n",
        "        self.train_f1 =[]\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        print(f'Epoch : {self.current_epoch} Loss : {np.mean(self.val_loss)} F1 : {np.mean(self.val_f1)}')\n",
        "        self.val_f1 =[]\n",
        "        self.val_loss = []\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr = self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        sents, tags, lengths = batch['sent'], batch['tag'], batch['lengths']\n",
        "        mask = (tags != PAD_ID)\n",
        "        logits = self(sents, lengths)\n",
        "        _ , preds = torch.max(logits, dim = 1)\n",
        "\n",
        "        ## calculate metrics\n",
        "        f1_score, precision, recall = self.calculate_metrics(preds, tags, mask)\n",
        "        self.test_f1.append(f1_score)\n",
        "        self.test_precision.append(precision)\n",
        "        self.test_recall.append(recall)\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        print(f'F1 : {np.mean(self.test_f1)} Precision : {np.mean(self.test_precision)} Recall : {np.mean(self.test_recall)}')\n",
        "        self.test_f1 = []\n",
        "        self.test_precision = []\n",
        "        self.test_recall = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF3hNnT3PKkb"
      },
      "outputs": [],
      "source": [
        "# model= NERModel(vocab_size = len(word2idx),\n",
        "#                 emb_dim = 100,\n",
        "#                 hidden_dim = 64,\n",
        "#                 n_tags = len(tag2idx),\n",
        "#                 learning_rate = 1e-3,\n",
        "#                 dropout = 0.3,\n",
        "#                 bidirectional = True,\n",
        "#                 n_layers = 2,\n",
        "#                 use_pretrained=True\n",
        "#                 )\n",
        "\n",
        "# logits = model(example['sent'], example['lengths'], verbose = True)\n",
        "# true_label = example['tag']\n",
        "# print(f\"True label shape : {true_label.shape}\")\n",
        "# loss = model.loss_fn(logits, true_label)\n",
        "# print(loss)\n",
        "# _ , pred_label = torch.max(logits, dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpfUCHOmPKkb",
        "outputId": "9ae9d41a-0058-453c-f70b-5d114d07e8bd",
        "colab": {
          "referenced_widgets": [
            "314023cb44b240ac9cc98f64679e45cc",
            "deb1c29c341743d6b59bb418e50e6f07",
            "ca65f8e8e7d74aa29bdec279ae113dc3",
            "bd3dfc798492438cba5113baaeab53c3",
            "43b49ae56c5c4f1493fad02af1f51b8d",
            "78a6ada76e474a3ab2125698f0cbc34d",
            "40baf283b58d4f2d8f76b786309066fa"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /home/saurabh/mydata/checkpoints_logs exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name       | Type             | Params\n",
            "------------------------------------------------\n",
            "0 | loss_fn    | CrossEntropyLoss | 0     \n",
            "1 | embedding  | Embedding        | 3.5 M \n",
            "2 | lstm       | LSTM             | 403 K \n",
            "3 | hidden2tag | Linear           | 3.6 K \n",
            "4 | relu       | ReLU             | 0     \n",
            "5 | dropout    | Dropout          | 0     \n",
            "------------------------------------------------\n",
            "3.9 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.9 M     Total params\n",
            "15.699    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "314023cb44b240ac9cc98f64679e45cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 0 Loss : 2.8791871070861816 F1 : 0.1052684133559239\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: __PAD__ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "deb1c29c341743d6b59bb418e50e6f07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca65f8e8e7d74aa29bdec279ae113dc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 0 Loss : 0.12112652530272802 F1 : 0.8512130343027688\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd3dfc798492438cba5113baaeab53c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 1 Loss : 0.1085385206176175 F1 : 0.8686529266721666\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43b49ae56c5c4f1493fad02af1f51b8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 2 Loss : 0.09986486341390345 F1 : 0.8730747940696316\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78a6ada76e474a3ab2125698f0cbc34d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 3 Loss : 0.09947697059147888 F1 : 0.8763026366238288\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40baf283b58d4f2d8f76b786309066fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 4 Loss : 0.10616053546054496 F1 : 0.8730404871427234\n"
          ]
        }
      ],
      "source": [
        "## Model Training\n",
        "model= NERModel(vocab_size = len(word2idx),\n",
        "                emb_dim = 100,\n",
        "                hidden_dim = 100,\n",
        "                n_tags = len(tag2idx),\n",
        "                learning_rate = 1e-3,\n",
        "                dropout = 0.5,\n",
        "                bidirectional = True,\n",
        "                n_layers = 2,\n",
        "                use_pretrained= True\n",
        "                )\n",
        "\n",
        "callbacks = pl.callbacks.ModelCheckpoint(dirpath = \"checkpoints_logs\",\n",
        "                                         filename = '{epoch}-{val_loss:.2f}-{val_f1:.2f}',\n",
        "                                          mode = \"min\",\n",
        "                                          monitor = \"val_loss\",\n",
        "                                          save_last = True,\n",
        "                                          save_top_k=-1)\n",
        "\n",
        "\n",
        "trainer = pl.Trainer(accelerator= \"gpu\",\n",
        "           max_epochs=5,\n",
        "           check_val_every_n_epoch = 1,\n",
        "           callbacks = [callbacks])\n",
        "\n",
        "trainer.fit(model, train_dl, val_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiclvFEdPKkc",
        "outputId": "702f8d5e-4627-46f9-9633-abeff3bf47b6",
        "colab": {
          "referenced_widgets": [
            "821cc421292348c2acaa74aad7646c34"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "821cc421292348c2acaa74aad7646c34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 : 0.8752422424553526 Precision : 0.8752619576276566 Recall : 0.8756151814430803\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{}]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## F1 : 0.8752422424553526 Precision : 0.8752619576276566 Recall : 0.8756151814430803\n",
        "trainer.test(model, dataloaders= test_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWOdTE8vPKkc"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx2R2sBdPKkd"
      },
      "outputs": [],
      "source": [
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVFPT1JrPKkd"
      },
      "outputs": [],
      "source": [
        "def process_data(text):\n",
        "    text = text.strip().split(\" \")\n",
        "    lengths = len(text)\n",
        "    encoded = []\n",
        "    for w in text:\n",
        "        encoded.append(word2idx.get(w, PAD_ID))\n",
        "\n",
        "    text_tensor = torch.tensor(encoded).view(1, -1)\n",
        "    lengths = torch.tensor([lengths])\n",
        "    return text_tensor, lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GQxjUkiPKkd",
        "outputId": "b0fa0eeb-c293-4e16-b34d-44c4ed49b453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 27]) torch.Size([1])\n",
            "Chinese     -->  B-gpe --> B-gpe\n",
            "worker      -->  O     --> O    \n",
            "sews        -->  O     --> O    \n",
            "clothing    -->  O     --> O    \n",
            "at          -->  O     --> O    \n",
            "a           -->  O     --> O    \n",
            "garment     -->  O     --> O    \n",
            "factory     -->  O     --> O    \n",
            "in          -->  O     --> O    \n",
            "Beijing     -->  B-geo --> B-geo\n",
            "China       -->  I-geo --> I-geo\n",
            "is          -->  O     --> O    \n",
            "criticizing  -->  O     --> O    \n",
            "the         -->  O     --> O    \n",
            "European    -->  B-org --> B-org\n",
            "Union       -->  I-org --> I-org\n",
            "'s          -->  O     --> O    \n",
            "decision    -->  O     --> O    \n",
            "to          -->  O     --> O    \n",
            "investigate  -->  O     --> O    \n",
            "surging     -->  O     --> O    \n",
            "imports     -->  O     --> O    \n",
            "of          -->  O     --> O    \n",
            "Chinese     -->  B-gpe --> B-gpe\n",
            "textile     -->  O     --> O    \n",
            "products    -->  O     --> O    \n",
            ".\n",
            "          -->  O     --> O    \n"
          ]
        }
      ],
      "source": [
        "i = random.choices(list(range(len(test_sents))))[0]\n",
        "text = test_sents[i]\n",
        "true_label = test_tags[i].strip().split(\" \")\n",
        "text_tensor, lengths = process_data(text)\n",
        "print(text_tensor.shape, lengths.shape)\n",
        "\n",
        "logits = model(text_tensor, lengths)\n",
        "_ , preds = torch.max(logits, dim = 1)\n",
        "preds = preds.numpy()[0]\n",
        "pred_labels = [idx2tag[p] for p in preds]\n",
        "\n",
        "for w, p, t in zip(text.split(\" \"), pred_labels, true_label):\n",
        "    print(f\"{w:<10}  -->  {p:<5} --> {t:<5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP3NK1eGPKkr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lighting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}