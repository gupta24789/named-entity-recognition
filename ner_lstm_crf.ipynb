{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gupta24789/named-entity-recognition/blob/main/ner_lstm_crf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNAfvIPRk_UD"
      },
      "outputs": [],
      "source": [
        "# !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip -d embeddings/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyee7DoEk_UG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxNDWAipk_UG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "from torch import optim\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from seqeval import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqTRc6Nek_UH"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZmPIhwZk_UJ"
      },
      "outputs": [],
      "source": [
        "# Path('data/train').mkdir(parents = True, exist_ok= True)\n",
        "# Path('data/val').mkdir(parents = True, exist_ok= True)\n",
        "# Path('data/test').mkdir(parents = True, exist_ok= True)\n",
        "\n",
        "# os.system(\"cd data/train && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/train/sentences.txt\")\n",
        "# os.system(\"cd data/train && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/train/labels.txt\")\n",
        "# os.system(\"cd data/val && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/val/sentences.txt\")\n",
        "# os.system(\"cd data/val && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/val/labels.txt\")\n",
        "# os.system(\"cd data/test && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/test/sentences.txt\")\n",
        "# os.system(\"cd data/test && wget https://raw.githubusercontent.com/gupta24789/named-entity-recognition/main/data/test/labels.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVVbJmuYk_UJ"
      },
      "source": [
        "## Set Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTXh_9GNk_UK",
        "outputId": "6b031976-9552-4784-e5d2-dd9e103583cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 121\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "121"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seed = 121\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "pl.seed_everything(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rg7bAFmk_UL"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkX4_YaKk_UL"
      },
      "outputs": [],
      "source": [
        "## train\n",
        "train_sents = open(\"data/train/sentences.txt\",\"r\").readlines()\n",
        "train_tags = open(\"data/train/labels.txt\",\"r\").readlines()\n",
        "## val\n",
        "val_sents = open(\"data/val/sentences.txt\",\"r\").readlines()\n",
        "val_tags = open(\"data/val/labels.txt\",\"r\").readlines()\n",
        "## test\n",
        "test_sents = open(\"data/test/sentences.txt\",\"r\").readlines()\n",
        "test_tags = open(\"data/test/labels.txt\",\"r\").readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ov5NnveKk_UM",
        "outputId": "9cd7145f-fe7a-4f16-bb4e-24102b27dd94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\\n',\n",
              " 'Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"\\n']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_sents[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUDir9nDk_UM",
        "outputId": "2eff4e9c-f4ef-467d-8ad9-db70829d4f9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\\n',\n",
              " 'O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O\\n']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_tags[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4q-_xRYk_UN"
      },
      "outputs": [],
      "source": [
        "X_train = [sent.strip().split(\" \") for sent in train_sents]\n",
        "y_train = [tag.strip().split(\" \") for tag in train_tags]\n",
        "\n",
        "X_val = [sent.strip().split(\" \") for sent in val_sents]\n",
        "y_val = [tag.strip().split(\" \") for tag in val_tags]\n",
        "\n",
        "X_test = [sent.strip().split(\" \") for sent in test_sents]\n",
        "y_test = [tag.strip().split(\" \") for tag in test_tags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "045CNBUHk_UN",
        "outputId": "decc04b8-7e64-43e4-9d8a-952f1973e74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London',\n",
            "  'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the',\n",
            "  'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.'],\n",
            " ['Families', 'of', 'soldiers', 'killed', 'in', 'the', 'conflict', 'joined',\n",
            "  'the', 'protesters', 'who', 'carried', 'banners', 'with', 'such', 'slogans',\n",
            "  'as', '\"', 'Bush', 'Number', 'One', 'Terrorist', '\"', 'and', '\"', 'Stop',\n",
            "  'the', 'Bombings', '.', '\"']]\n"
          ]
        }
      ],
      "source": [
        "pprint((X_train[:2]), compact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjecjDtSk_UO",
        "outputId": "8777e254-11a0-425c-e620-4672ded7b4cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O',\n",
            "  'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']]\n"
          ]
        }
      ],
      "source": [
        "pprint(y_train[:1], compact=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmceQeldk_UO"
      },
      "source": [
        "## Create Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L4XOQx2k_UP",
        "outputId": "1939e388-e659-4527-feb3-564831c5ee9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size : 35180\n",
            "tags : 18\n",
            "{'B-art': 11,\n",
            " 'B-eve': 1,\n",
            " 'B-geo': 3,\n",
            " 'B-gpe': 5,\n",
            " 'B-nat': 16,\n",
            " 'B-org': 6,\n",
            " 'B-per': 17,\n",
            " 'B-tim': 13,\n",
            " 'I-art': 15,\n",
            " 'I-eve': 14,\n",
            " 'I-geo': 2,\n",
            " 'I-gpe': 9,\n",
            " 'I-nat': 10,\n",
            " 'I-org': 4,\n",
            " 'I-per': 7,\n",
            " 'I-tim': 8,\n",
            " 'O': 12,\n",
            " '__PAD__': 0}\n",
            "PAD ID : 0\n"
          ]
        }
      ],
      "source": [
        "special_words = ['__PAD__','__UNK__']\n",
        "vocab = list(set(itertools.chain.from_iterable(X_train + X_val + X_test)))\n",
        "vocab = special_words + vocab\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "## TAGS\n",
        "tags = list(set(itertools.chain.from_iterable(y_train)))\n",
        "tags = ['__PAD__'] + tags\n",
        "tag2idx = {w:i for i,w in enumerate(tags)}\n",
        "idx2tag = {i:w for w,i in tag2idx.items()}\n",
        "\n",
        "\n",
        "print(f\"vocab size : {len(vocab)}\")\n",
        "print(f\"tags : {len(tag2idx)}\")\n",
        "pprint(tag2idx, compact=True)\n",
        "\n",
        "PAD_ID = word2idx['__PAD__']\n",
        "UNK_ID = word2idx['__UNK__']\n",
        "\n",
        "print(f\"PAD ID : {PAD_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxO5dJs5k_UP"
      },
      "source": [
        "## Encode sent & tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQLAGmsUk_UQ"
      },
      "outputs": [],
      "source": [
        "def to_sent_number(sent_list):\n",
        "    encoded = []\n",
        "    for w in sent_list:\n",
        "        encoded.append(word2idx.get(w, UNK_ID))\n",
        "    return encoded\n",
        "\n",
        "\n",
        "def to_tag_number(tag_list):\n",
        "    encoded = []\n",
        "    for tag in tag_list:\n",
        "        encoded.append(tag2idx[tag])\n",
        "    return encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmH3PbC_k_UQ"
      },
      "outputs": [],
      "source": [
        "X_train_encoded = [to_sent_number(sent) for sent in X_train]\n",
        "y_train_encoded = [to_tag_number(tags) for tags in y_train]\n",
        "\n",
        "X_val_encoded = [to_sent_number(sent) for sent in X_val]\n",
        "y_val_encoded = [to_tag_number(tags) for tags in y_val]\n",
        "\n",
        "X_test_encoded = [to_sent_number(sent) for sent in X_test]\n",
        "y_test_encoded = [to_tag_number(tags) for tags in y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAObjy9jk_UQ",
        "outputId": "92629720-f808-4a7d-8a4e-79312a8bb85a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[5177, 16237, 7828, 164, 1867, 6988, 33846, 18600, 3536, 21836, 15143, 32322,\n",
            "  27531, 30680, 9422, 21836, 5959, 16237, 303, 25128, 1025, 35123, 25622,\n",
            "  16704],\n",
            " [13320, 16237, 2102, 807, 32322, 21836, 31673, 31639, 21836, 30604, 22262,\n",
            "  25740, 29048, 9555, 13328, 28557, 26299, 1963, 6110, 32088, 30569, 12047,\n",
            "  1963, 30680, 1963, 29707, 21836, 27699, 16704, 1963]]\n"
          ]
        }
      ],
      "source": [
        "pprint(X_train_encoded[:2], compact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1-s4IKtk_UR",
        "outputId": "5299bddc-7ee6-4135-ef0d-80760020294f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[12, 12, 12, 12, 12, 12, 3, 12, 12, 12, 12, 12, 3, 12, 12, 12, 12, 12, 5, 12,\n",
            "  12, 12, 12, 12],\n",
            " [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 17,\n",
            "  12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]]\n"
          ]
        }
      ],
      "source": [
        "pprint(y_train_encoded[:2], compact=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0jgOjQsk_UR"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtN3rQ03k_UR"
      },
      "outputs": [],
      "source": [
        "def custom_collate(batch):\n",
        "    sent = [torch.tensor(item[0]) for item in batch]\n",
        "    tag = [torch.tensor(item[1]) for item in batch]\n",
        "    lengths = torch.tensor([len(item[0]) for item in batch])\n",
        "\n",
        "    padded_sent = nn.utils.rnn.pad_sequence(sent, batch_first=True, padding_value=PAD_ID)\n",
        "    padded_tag = nn.utils.rnn.pad_sequence(tag, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    batch = {\"sent\": padded_sent, \"tag\": padded_tag, \"lengths\": lengths}\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yuc5D2chk_US"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(list(zip(X_train_encoded,y_train_encoded)), batch_size = 2, shuffle = False, collate_fn = custom_collate )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPWVGJyOk_US",
        "outputId": "3a2bb2fb-f859-4132-f792-236d205069af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([2, 30]), torch.Size([2, 30]), torch.Size([2]))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = next(iter(train_dl))\n",
        "example['sent'].shape, example['tag'].shape,  example['lengths'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCSDvYYOk_UT",
        "outputId": "d8895839-7d02-4dc3-e8e6-279062221959"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 5177, 16237,  7828,   164,  1867,  6988, 33846, 18600,  3536, 21836,\n",
              "         15143, 32322, 27531, 30680,  9422, 21836,  5959, 16237,   303, 25128,\n",
              "          1025, 35123, 25622, 16704,     0,     0,     0,     0,     0,     0],\n",
              "        [13320, 16237,  2102,   807, 32322, 21836, 31673, 31639, 21836, 30604,\n",
              "         22262, 25740, 29048,  9555, 13328, 28557, 26299,  1963,  6110, 32088,\n",
              "         30569, 12047,  1963, 30680,  1963, 29707, 21836, 27699, 16704,  1963]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example['sent']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFT5-vKok_UU",
        "outputId": "d034c905-a771-4cda-bae6-3c4e5614e6f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[12, 12, 12, 12, 12, 12,  3, 12, 12, 12, 12, 12,  3, 12, 12, 12, 12, 12,\n",
              "          5, 12, 12, 12, 12, 12,  0,  0,  0,  0,  0,  0],\n",
              "        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
              "         17, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example['tag']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC2VFYlIk_UU",
        "outputId": "03e9fa22-8d1d-4816-f08a-a4ab75a0d3db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([24, 30])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example['lengths']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoVCmLdfk_UV"
      },
      "outputs": [],
      "source": [
        "## dataloaders\n",
        "batch_size = 32\n",
        "train_dl = DataLoader(list(zip(X_train_encoded,y_train_encoded)), batch_size = batch_size, shuffle = True, collate_fn = custom_collate )\n",
        "val_dl = DataLoader(list(zip(X_val_encoded,y_val_encoded)), batch_size = batch_size, shuffle = False, collate_fn = custom_collate )\n",
        "test_dl = DataLoader(list(zip(X_test_encoded,y_test_encoded)), batch_size = batch_size, shuffle = False, collate_fn = custom_collate )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wHrvcHWk_UV"
      },
      "source": [
        "## Pretrained Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZED9LJxSk_UV",
        "outputId": "9f469a87-006e-48a8-a7f5-9b116a37816d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3092/624670142.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  return torch.tensor(df_list)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([35180, 100])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Pretrained Vectors\n",
        "def load_pretrain_emb(filepath):\n",
        "    lines = open(filepath,\"r\").readlines()\n",
        "    embedd_dict = {}\n",
        "    for line in lines:\n",
        "        if len(line)>0:\n",
        "            tokens = line.strip().split(\" \")\n",
        "            word = tokens[0]\n",
        "            vec = tokens[1:]\n",
        "            vec = np.array(vec).astype(float)\n",
        "            embedd_dict[word]= vec\n",
        "\n",
        "    return embedd_dict\n",
        "\n",
        "def build_pretrain_embedding(filepath, vocab, emb_dim):\n",
        "    embedd_dict = load_pretrain_emb(filepath)\n",
        "\n",
        "    df_list = []\n",
        "\n",
        "    for w,i in vocab.items():\n",
        "        if w in embedd_dict:\n",
        "            df_list.append(torch.tensor(embedd_dict[w]))\n",
        "        elif w.lower() in embedd_dict:\n",
        "            df_list.append(embedd_dict[w.lower()])\n",
        "        else:\n",
        "            random_vec = np.random.normal(size = (emb_dim))\n",
        "            df_list.append(random_vec)\n",
        "\n",
        "\n",
        "    return torch.tensor(df_list)\n",
        "\n",
        "\n",
        "\n",
        "weights = build_pretrain_embedding(\"embeddings/glove.6B.100d.txt\", word2idx, emb_dim=100)\n",
        "weights.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHKhxmVAk_UW"
      },
      "source": [
        "## CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQdkhgXWk_UW"
      },
      "outputs": [],
      "source": [
        "START_TAG = -2\n",
        "STOP_TAG = -1\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec, m_size):\n",
        "    \"\"\"\n",
        "    calculate log of exp sum\n",
        "    args:\n",
        "        vec (batch_size, vanishing_dim, hidden_dim) : input tensor\n",
        "        m_size : hidden_dim\n",
        "    return:\n",
        "        batch_size, hidden_dim\n",
        "    \"\"\"\n",
        "    _, idx = torch.max(vec, 1)  # B * 1 * M\n",
        "    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\n",
        "    return max_score.view(-1, m_size) + torch.log(torch.sum(torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1,\n",
        "                                                                                                                m_size)  # B * M\n",
        "\n",
        "\n",
        "class CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, tagset_size, gpu):\n",
        "        super(CRF, self).__init__()\n",
        "        print(\"build CRF...\")\n",
        "        self.gpu = gpu\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of transitioning from i to j.\n",
        "        self.tagset_size = tagset_size\n",
        "        # # We add 2 here, because of START_TAG and STOP_TAG\n",
        "        # # transitions (f_tag_size, t_tag_size), transition value from f_tag to t_tag\n",
        "        init_transitions = torch.zeros(self.tagset_size + 2, self.tagset_size + 2)\n",
        "        init_transitions[:, START_TAG] = -10000.0\n",
        "        init_transitions[STOP_TAG, :] = -10000.0\n",
        "        init_transitions[:, 0] = -10000.0\n",
        "        init_transitions[0, :] = -10000.0\n",
        "        # if self.gpu:\n",
        "        #     init_transitions = init_transitions.cuda()\n",
        "        self.transitions = nn.Parameter(init_transitions)\n",
        "\n",
        "        # self.transitions = nn.Parameter(torch.Tensor(self.tagset_size+2, self.tagset_size+2))\n",
        "        # self.transitions.data.zero_()\n",
        "\n",
        "    def _calculate_PZ(self, feats, mask):\n",
        "        \"\"\"\n",
        "            input:\n",
        "                feats: (batch, seq_len, self.tag_size+2)\n",
        "                masks: (batch, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size = feats.size(0)\n",
        "        seq_len = feats.size(1)\n",
        "        tag_size = feats.size(2)\n",
        "        # print feats.view(seq_len, tag_size)\n",
        "        assert (tag_size == self.tagset_size + 2)\n",
        "        mask = mask.transpose(1, 0).contiguous()\n",
        "        ins_num = seq_len * batch_size\n",
        "        ## be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n",
        "        feats = feats.transpose(1, 0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n",
        "        ## need to consider start\n",
        "        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n",
        "        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n",
        "        # build iter\n",
        "        seq_iter = enumerate(scores)\n",
        "        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n",
        "        # only need start from start_tag\n",
        "        partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size, 1)  # bat_size * to_target_size\n",
        "\n",
        "        ## add start score (from start to all tag, duplicate to batch_size)\n",
        "        # partition = partition + self.transitions[START_TAG,:].view(1, tag_size, 1).expand(batch_size, tag_size, 1)\n",
        "        # iter over last scores\n",
        "        for idx, cur_values in seq_iter:\n",
        "            # previous to_target is current from_target\n",
        "            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n",
        "            # cur_values: bat_size * from_target * to_target\n",
        "\n",
        "            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size,\n",
        "                                                                                                  tag_size)\n",
        "            cur_partition = log_sum_exp(cur_values, tag_size)\n",
        "            # print cur_partition.data\n",
        "\n",
        "            # (bat_size * from_target * to_target) -> (bat_size * to_target)\n",
        "            # partition = utils.switch(partition, cur_partition, mask[idx].view(bat_size, 1).expand(bat_size, self.tagset_size)).view(bat_size, -1)\n",
        "            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n",
        "\n",
        "            ## effective updated partition part, only keep the partition value of mask value = 1\n",
        "            masked_cur_partition = cur_partition.masked_select(mask_idx)\n",
        "            ## let mask_idx broadcastable, to disable warning\n",
        "            mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n",
        "\n",
        "            ## replace the partition where the maskvalue=1, other partition value keeps the same\n",
        "            partition.masked_scatter_(mask_idx, masked_cur_partition)\n",
        "        # until the last state, add transition score for all partition (and do log_sum_exp) then select the value in STOP_TAG\n",
        "        cur_values = self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size,\n",
        "                                                                         tag_size) + partition.contiguous().view(\n",
        "            batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n",
        "        cur_partition = log_sum_exp(cur_values, tag_size)\n",
        "        final_partition = cur_partition[:, STOP_TAG]\n",
        "        return final_partition.sum(), scores\n",
        "\n",
        "    def _viterbi_decode(self, feats, mask):\n",
        "        \"\"\"\n",
        "            input:\n",
        "                feats: (batch, seq_len, self.tag_size+2)\n",
        "                mask: (batch, seq_len)\n",
        "            output:\n",
        "                decode_idx: (batch, seq_len) decoded sequence\n",
        "                path_score: (batch, 1) corresponding score for each sequence (to be implementated)\n",
        "        \"\"\"\n",
        "        batch_size = feats.size(0)\n",
        "        seq_len = feats.size(1)\n",
        "        tag_size = feats.size(2)\n",
        "        assert (tag_size == self.tagset_size + 2)\n",
        "        ## calculate sentence length for each sentence\n",
        "        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n",
        "        ## mask to (seq_len, batch_size)\n",
        "        mask = mask.transpose(1, 0).contiguous()\n",
        "        ins_num = seq_len * batch_size\n",
        "        ## be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n",
        "        feats = feats.transpose(1, 0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n",
        "        ## need to consider start\n",
        "        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n",
        "        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n",
        "\n",
        "        # build iter\n",
        "        seq_iter = enumerate(scores)\n",
        "        ## record the position of best score\n",
        "        back_points = list()\n",
        "        partition_history = list()\n",
        "        ##  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n",
        "        # mask = 1 + (-1)*mask\n",
        "        mask = (1 - mask.long()).byte()\n",
        "        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n",
        "        # only need start from start_tag\n",
        "        partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size)  # bat_size * to_target_size\n",
        "        # print \"init part:\",partition.size()\n",
        "        partition_history.append(partition)\n",
        "        # iter over last scores\n",
        "        for idx, cur_values in seq_iter:\n",
        "            # previous to_target is current from_target\n",
        "            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n",
        "            # cur_values: batch_size * from_target * to_target\n",
        "            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size,\n",
        "                                                                                                  tag_size)\n",
        "            ## forscores, cur_bp = torch.max(cur_values[:,:-2,:], 1) # do not consider START_TAG/STOP_TAG\n",
        "            # print \"cur value:\", cur_values.size()\n",
        "            partition, cur_bp = torch.max(cur_values, 1)\n",
        "            # print \"partsize:\",partition.size()\n",
        "            # exit(0)\n",
        "            # print partition\n",
        "            # print cur_bp\n",
        "            # print \"one best, \",idx\n",
        "            partition_history.append(partition)\n",
        "            ## cur_bp: (batch_size, tag_size) max source score position in current tag\n",
        "            ## set padded label as 0, which will be filtered in post processing\n",
        "            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n",
        "            back_points.append(cur_bp)\n",
        "        # exit(0)\n",
        "        ### add score to final STOP_TAG\n",
        "        partition_history = torch.cat(partition_history, 0).view(seq_len, batch_size, -1).transpose(1,\n",
        "                                                                                                    0).contiguous()  ## (batch_size, seq_len. tag_size)\n",
        "        ### get the last position for each setences, and select the last partitions using gather()\n",
        "        last_position = length_mask.view(batch_size, 1, 1).expand(batch_size, 1, tag_size) - 1\n",
        "        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size, tag_size, 1)\n",
        "        ### calculate the score from last partition to end state (and then select the STOP_TAG from it)\n",
        "        last_values = last_partition.expand(batch_size, tag_size, tag_size) + self.transitions.view(1, tag_size,\n",
        "                                                                                                    tag_size).expand(\n",
        "            batch_size, tag_size, tag_size)\n",
        "        _, last_bp = torch.max(last_values, 1)\n",
        "        pad_zero = autograd.Variable(torch.zeros(batch_size, tag_size)).long()\n",
        "        if self.gpu:\n",
        "            pad_zero = pad_zero.cuda()\n",
        "        back_points.append(pad_zero)\n",
        "        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size)\n",
        "\n",
        "        ## select end ids in STOP_TAG\n",
        "        pointer = last_bp[:, STOP_TAG]\n",
        "        insert_last = pointer.contiguous().view(batch_size, 1, 1).expand(batch_size, 1, tag_size)\n",
        "        back_points = back_points.transpose(1, 0).contiguous()\n",
        "        ## move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values\n",
        "        # print \"lp:\",last_position\n",
        "        # print \"il:\",insert_last\n",
        "        back_points.scatter_(1, last_position, insert_last)\n",
        "        # print \"bp:\",back_points\n",
        "        # exit(0)\n",
        "        back_points = back_points.transpose(1, 0).contiguous()\n",
        "        ## decode from the end, padded position ids are 0, which will be filtered if following evaluation\n",
        "        decode_idx = autograd.Variable(torch.LongTensor(seq_len, batch_size))\n",
        "        if self.gpu:\n",
        "            decode_idx = decode_idx.cuda()\n",
        "        decode_idx[-1] = pointer.detach()\n",
        "        for idx in range(len(back_points) - 2, -1, -1):\n",
        "            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n",
        "            decode_idx[idx] = pointer.detach().view(batch_size)\n",
        "        path_score = None\n",
        "        decode_idx = decode_idx.transpose(1, 0)\n",
        "        return path_score, decode_idx\n",
        "\n",
        "    def _score_sentence(self, scores, mask, tags):\n",
        "        \"\"\"\n",
        "            input:\n",
        "                scores: variable (seq_len, batch, tag_size, tag_size)\n",
        "                mask: (batch, seq_len)\n",
        "                tags: tensor  (batch, seq_len)\n",
        "            output:\n",
        "                score: sum of score for gold sequences within whole batch\n",
        "        \"\"\"\n",
        "        # Gives the score of a provided tag sequence\n",
        "        batch_size = scores.size(1)\n",
        "        seq_len = scores.size(0)\n",
        "        tag_size = scores.size(2)\n",
        "        ## convert tag value into a new format, recorded label bigram information to index\n",
        "        new_tags = autograd.Variable(torch.LongTensor(batch_size, seq_len))\n",
        "        if self.gpu:\n",
        "            new_tags = new_tags.cuda()\n",
        "        for idx in range(seq_len):\n",
        "            if idx == 0:\n",
        "                ## start -> first score\n",
        "                new_tags[:, 0] = (tag_size - 2) * tag_size + tags[:, 0]\n",
        "\n",
        "            else:\n",
        "                new_tags[:, idx] = tags[:, idx - 1] * tag_size + tags[:, idx]\n",
        "\n",
        "        ## transition for label to STOP_TAG\n",
        "        end_transition = self.transitions[:, STOP_TAG].contiguous().view(1, tag_size).expand(batch_size, tag_size)\n",
        "        ## length for batch,  last word position = length - 1\n",
        "        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n",
        "        ## index the label id of last word\n",
        "        end_ids = torch.gather(tags, 1, length_mask - 1)\n",
        "\n",
        "        ## index the transition score for end_id to STOP_TAG\n",
        "        end_energy = torch.gather(end_transition, 1, end_ids)\n",
        "\n",
        "        ## convert tag as (seq_len, batch_size, 1)\n",
        "        new_tags = new_tags.transpose(1, 0).contiguous().view(seq_len, batch_size, 1)\n",
        "        ### need convert tags id to search from 400 positions of scores\n",
        "        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(seq_len,\n",
        "                                                                                         batch_size)  # seq_len * bat_size\n",
        "        ## mask transpose to (seq_len, batch_size)\n",
        "        tg_energy = tg_energy.masked_select(mask.transpose(1, 0))\n",
        "\n",
        "        # ## calculate the score from START_TAG to first label\n",
        "        # start_transition = self.transitions[START_TAG,:].view(1, tag_size).expand(batch_size, tag_size)\n",
        "        # start_energy = torch.gather(start_transition, 1, tags[0,:])\n",
        "\n",
        "        ## add all score together\n",
        "        # gold_score = start_energy.sum() + tg_energy.sum() + end_energy.sum()\n",
        "        gold_score = tg_energy.sum() + end_energy.sum()\n",
        "        return gold_score\n",
        "\n",
        "    def neg_log_likelihood_loss(self, feats, mask, tags):\n",
        "        # nonegative log likelihood\n",
        "        batch_size = feats.size(0)\n",
        "        forward_score, scores = self._calculate_PZ(feats, mask)\n",
        "        gold_score = self._score_sentence(scores, mask, tags)\n",
        "        # print \"batch, f:\", forward_score.data[0], \" g:\", gold_score.data[0], \" dis:\", forward_score.data[0] - gold_score.data[0]\n",
        "        # exit(0)\n",
        "        return forward_score - gold_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0btKsRuvk_UY"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV-yY02Dk_UY"
      },
      "outputs": [],
      "source": [
        "class NERModel(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    if you will you NLLLoss then you have to use log_softmax in forward else use CrossEntropy\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_tags, learning_rate, dropout, bidirectional = False, n_layers = 1, use_pretrained = True, use_crf = True, use_gpu = True):\n",
        "        super().__init__()\n",
        "        self.use_crf = use_crf\n",
        "        self.crf = CRF(tagset_size= n_tags, gpu= use_gpu)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # metrics\n",
        "        self.train_f1 = []\n",
        "        self.val_f1 = []\n",
        "        self.val_loss = []\n",
        "        self.test_f1 =[]\n",
        "        self.test_precision = []\n",
        "        self.test_recall = []\n",
        "\n",
        "        ## define loss\n",
        "        if self.use_crf:\n",
        "            self.loss_fn = self.crf.neg_log_likelihood_loss\n",
        "        else:\n",
        "            self.loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "\n",
        "\n",
        "        ## layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim= emb_dim, padding_idx= PAD_ID)\n",
        "        if use_pretrained:\n",
        "            self.embedding.weight.data.copy_(weights)\n",
        "        else:\n",
        "            self.embedding.weight.data.copy_(torch.from_numpy(self.random_embedding(vocab_size, emb_dim)))\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=bidirectional, dropout = dropout, num_layers = n_layers)\n",
        "\n",
        "        if self.use_crf:\n",
        "            self.hidden2tag = nn.Linear(hidden_dim * 2, n_tags + 2)\n",
        "        else:\n",
        "            self.hidden2tag = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, n_tags)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def random_embedding(self, vocab_size, embedding_dim):\n",
        "        pretrain_emb = np.empty([vocab_size, embedding_dim])\n",
        "        scale = np.sqrt(3.0 / embedding_dim)\n",
        "        for index in range(1, vocab_size):\n",
        "            pretrain_emb[index, :] = np.random.uniform(-scale, scale, [1, embedding_dim])\n",
        "        return pretrain_emb\n",
        "\n",
        "    def forward(self, sent, lengths, verbose = False):\n",
        "\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(sent, lengths.to('cpu'), batch_first = True, enforce_sorted = False)\n",
        "        x, xlengths = nn.utils.rnn.pad_packed_sequence(packed_input, batch_first = True)\n",
        "\n",
        "        ## layers\n",
        "        embedded = self.embedding(x)\n",
        "        # embedded : [batch size, seq_len, emb dim]\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        # output : [batch size,seq_len,  2*hidden dim]\n",
        "        # hidden : [bidirectional * num layers, batch size, hidden dim]  ### if bidirectional == True then multiply by 2 else 1\n",
        "        # cell : [bidirectional * num layers, batch size, hidden dim]\n",
        "\n",
        "\n",
        "        output = self.dropout(output)\n",
        "        logits = self.hidden2tag(output)\n",
        "         # logits : [ batch size, seq_len, num_class]\n",
        "        if self.use_crf:\n",
        "            logits = logits\n",
        "        else:\n",
        "            logits = logits.permute(0,2,1)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Sent : {sent.shape}\")\n",
        "            print(f'length : {lengths.shape}')\n",
        "            print(f'x : {x.shape}')\n",
        "            print(f'xlengths : {xlengths.shape}')\n",
        "            print(f'embedded : {embedded.shape}')\n",
        "            print(f'output : {output.shape}')\n",
        "            print(f'hidden : {hidden.shape}')\n",
        "            print(f'cell : {cell.shape}')\n",
        "            print(f'logits : {logits.shape}')\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def calculate_metrics(self, y_true, y_pred, mask):\n",
        "\n",
        "        y_true = y_true  * mask\n",
        "        y_pred = y_pred * mask\n",
        "\n",
        "        ## metrics\n",
        "        y_true = y_true.cpu().numpy().tolist()\n",
        "        y_pred = y_pred.cpu().numpy().tolist()\n",
        "        y_true_label = [[idx2tag[tag] for tag in sent_tag] for sent_tag in y_true]\n",
        "        y_pred_label = [[idx2tag[tag] for tag in sent_tag] for sent_tag in y_pred]\n",
        "\n",
        "        f1_score = metrics.f1_score(y_true_label, y_pred_label)\n",
        "        precision = metrics.precision_score(y_true_label, y_pred_label)\n",
        "        recall = metrics.recall_score(y_true_label, y_pred_label)\n",
        "        return f1_score, precision, recall\n",
        "\n",
        "    def _shared_step(self, batch):\n",
        "        sents, tags, lengths = batch['sent'], batch['tag'], batch['lengths']\n",
        "        mask = (tags != PAD_ID)\n",
        "         # mask = (y_true != PAD_ID) * (y_true != tag2idx['O'])\n",
        "        logits = self(sents, lengths)\n",
        "\n",
        "        if self.use_crf:\n",
        "            loss = self.loss_fn(logits, mask, tags)\n",
        "            _, preds = self.crf._viterbi_decode(logits, mask)\n",
        "        else:\n",
        "            loss = self.loss_fn(logits, tags)\n",
        "            _ , preds = torch.max(logits, dim = 1)\n",
        "\n",
        "        ## calculate metrics\n",
        "        f1_score, precision, recall = self.calculate_metrics(preds, tags, mask)\n",
        "        return loss, f1_score, precision, recall\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        loss, f1_score, precision, recall = self._shared_step(batch)\n",
        "        self.train_f1.append(f1_score)\n",
        "        self.log_dict({\"train_loss\": loss, \"train_f1\": np.mean(self.train_f1)}, on_step = False, on_epoch = True, prog_bar=  True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        loss, f1_score, precision, recall = self._shared_step(batch)\n",
        "        self.val_f1.append(f1_score)\n",
        "        self.val_loss.append(loss.cpu().item())\n",
        "        self.log_dict({\"val_loss\": loss, \"val_f1\": np.mean(self.val_f1)}, on_step = False, on_epoch = True, prog_bar=  True)\n",
        "        return loss\n",
        "\n",
        "    def on_training_epoch_end(self):\n",
        "        self.train_f1 =[]\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        print(f'Epoch : {self.current_epoch} Loss : {np.mean(self.val_loss)} F1 : {np.mean(self.val_f1)}')\n",
        "        self.val_f1 =[]\n",
        "        self.val_loss = []\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr = self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        sents, tags, lengths = batch['sent'], batch['tag'], batch['lengths']\n",
        "        mask = (tags != PAD_ID)\n",
        "        logits = self(sents, lengths)\n",
        "\n",
        "        if self.use_crf:\n",
        "            _, preds = self.crf._viterbi_decode(logits, mask)\n",
        "        else:\n",
        "            _ , preds = torch.max(logits, dim = 1)\n",
        "\n",
        "        ## calculate metrics\n",
        "        f1_score, precision, recall = self.calculate_metrics(preds, tags, mask)\n",
        "        self.test_f1.append(f1_score)\n",
        "        self.test_precision.append(precision)\n",
        "        self.test_recall.append(recall)\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        print(f'F1 : {np.mean(self.test_f1)} Precision : {np.mean(self.test_precision)} Recall : {np.mean(self.test_recall)}')\n",
        "        self.test_f1 = []\n",
        "        self.test_precision = []\n",
        "        self.test_recall = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU8-IbUFk_UZ"
      },
      "outputs": [],
      "source": [
        "# model= NERModel(vocab_size = len(word2idx),\n",
        "#                 emb_dim = 100,\n",
        "#                 hidden_dim = 64,\n",
        "#                 n_tags = len(tag2idx),\n",
        "#                 learning_rate = 1e-3,\n",
        "#                 dropout = 0.3,\n",
        "#                 bidirectional = True,\n",
        "#                 n_layers = 2,\n",
        "#                 use_pretrained= True,\n",
        "#                 use_crf= True\n",
        "#                 )\n",
        "\n",
        "# logits = model(example['sent'], example['lengths'], verbose = True)\n",
        "# true_label = example['tag']\n",
        "# print(f\"True label shape : {true_label.shape}\")\n",
        "# mask = (true_label != PAD_ID)\n",
        "# loss = model.loss_fn(logits, mask, true_label)\n",
        "# print(loss)\n",
        "\n",
        "# scores, tag_seq = model.crf._viterbi_decode(logits, mask)\n",
        "# print(tag_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poXP5quZk_UZ",
        "outputId": "48449e3d-6efc-45db-cec4-93c816e80835",
        "colab": {
          "referenced_widgets": [
            "c4e57915cb9c451594346ace0f2289c7",
            "c0c3d06e90394530bd6d3b4018172d2f",
            "b6986bc2839c49b2aba87094c0cb20cf",
            "23ee0878e52c4e5395d95614a088d3bf",
            "221c8e51da6648b9ad2f870b794c2eb4",
            "e7be41091b6145138d5c3354b3746a1f",
            "cce2903e0bad41e39fce2e603b648e80"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name       | Type      | Params\n",
            "-----------------------------------------\n",
            "0 | crf        | CRF       | 400   \n",
            "1 | embedding  | Embedding | 3.5 M \n",
            "2 | lstm       | LSTM      | 403 K \n",
            "3 | hidden2tag | Linear    | 4.0 K \n",
            "4 | relu       | ReLU      | 0     \n",
            "5 | dropout    | Dropout   | 0     \n",
            "-----------------------------------------\n",
            "3.9 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.9 M     Total params\n",
            "15.702    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "build CRF...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4e57915cb9c451594346ace0f2289c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3092/57801177.py:154: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/cuda/Indexing.cu:1435.)\n",
            "  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 0 Loss : 2023.6969604492188 F1 : 0.0804396535387428\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0c3d06e90394530bd6d3b4018172d2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6986bc2839c49b2aba87094c0cb20cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 0 Loss : 73.45108615451389 F1 : 0.8654198407486087\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23ee0878e52c4e5395d95614a088d3bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 1 Loss : 59.49240234375 F1 : 0.8781658607859127\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "221c8e51da6648b9ad2f870b794c2eb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 2 Loss : 54.381664496527776 F1 : 0.884236453524691\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7be41091b6145138d5c3354b3746a1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 3 Loss : 53.05431640625 F1 : 0.8817809545168146\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cce2903e0bad41e39fce2e603b648e80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 4 Loss : 52.86887369791667 F1 : 0.8766334420321176\n"
          ]
        }
      ],
      "source": [
        "## Model Training\n",
        "model= NERModel(vocab_size = len(word2idx),\n",
        "                emb_dim = 100,\n",
        "                hidden_dim = 100,\n",
        "                n_tags = len(tag2idx),\n",
        "                learning_rate = 1e-3,\n",
        "                dropout = 0.5,\n",
        "                bidirectional = True,\n",
        "                n_layers = 2,\n",
        "                use_pretrained= True,\n",
        "                use_crf=True,\n",
        "                use_gpu= True\n",
        "                )\n",
        "\n",
        "callbacks = pl.callbacks.ModelCheckpoint(dirpath = \"checkpoints_logs\",\n",
        "                                         filename = '{epoch}-{val_loss:.2f}-{val_f1:.2f}',\n",
        "                                          mode = \"min\",\n",
        "                                          monitor = \"val_loss\",\n",
        "                                          save_last = True,\n",
        "                                          save_top_k=-1)\n",
        "\n",
        "\n",
        "trainer = pl.Trainer(accelerator= \"gpu\",\n",
        "           max_epochs=5,\n",
        "           check_val_every_n_epoch = 1,\n",
        "           callbacks = [callbacks])\n",
        "\n",
        "trainer.fit(model, train_dl, val_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VI9CH9zk_Ua",
        "outputId": "607d3839-3589-44e1-a6aa-bfb8a6628129",
        "colab": {
          "referenced_widgets": [
            "c042102301504fa39be3ca1a612e5665"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c042102301504fa39be3ca1a612e5665",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3092/57801177.py:154: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/cuda/Indexing.cu:1435.)\n",
            "  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 : 0.8806611207242312 Precision : 0.8815342369284661 Recall : 0.880136395051831\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{}]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## F1 : 0.8806611207242312 Precision : 0.8815342369284661 Recall : 0.880136395051831\n",
        "trainer.test(model, dataloaders= test_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrGkqBz7k_Ua"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AwtNOUJk_Um"
      },
      "outputs": [],
      "source": [
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhzyEokxk_Um"
      },
      "outputs": [],
      "source": [
        "def process_data(text):\n",
        "    text = text.strip().split(\" \")\n",
        "    lengths = len(text)\n",
        "    encoded = []\n",
        "    for w in text:\n",
        "        encoded.append(word2idx.get(w, PAD_ID))\n",
        "\n",
        "    text_tensor = torch.tensor(encoded).view(1, -1)\n",
        "    lengths = torch.tensor([lengths])\n",
        "    return text_tensor, lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANE28Ci_k_Um",
        "outputId": "c4891d1d-f776-4b3d-a707-0f81f8f961dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 44]) torch.Size([1])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The                   -->  O     --> O    \n",
            "Iranian               -->  B-gpe --> B-gpe\n",
            "spokesman             -->  O     --> O    \n",
            "'s                    -->  O     --> O    \n",
            "comments              -->  O     --> O    \n",
            "came                  -->  O     --> O    \n",
            "as                    -->  O     --> O    \n",
            "representatives       -->  O     --> O    \n",
            "of                    -->  O     --> O    \n",
            "Britain               -->  B-geo --> B-geo\n",
            ",                     -->  O     --> O    \n",
            "Germany               -->  B-geo --> B-geo\n",
            ",                     -->  O     --> O    \n",
            "France                -->  B-geo --> B-geo\n",
            ",                     -->  O     --> O    \n",
            "the                   -->  O     --> O    \n",
            "United                -->  B-geo --> B-geo\n",
            "States                -->  I-geo --> I-geo\n",
            "and                   -->  O     --> O    \n",
            "the                   -->  O     --> O    \n",
            "European              -->  B-org --> B-org\n",
            "Union                 -->  I-org --> I-org\n",
            "prepared              -->  O     --> O    \n",
            "for                   -->  O     --> O    \n",
            "a                     -->  O     --> O    \n",
            "meeting               -->  O     --> O    \n",
            "Monday                -->  B-tim --> B-tim\n",
            "in                    -->  O     --> O    \n",
            "London                -->  B-geo --> B-geo\n",
            "to                    -->  O     --> O    \n",
            "discuss               -->  O     --> O    \n",
            "whether               -->  O     --> O    \n",
            "to                    -->  O     --> O    \n",
            "refer                 -->  O     --> O    \n",
            "Iran                  -->  B-geo --> B-geo\n",
            "to                    -->  O     --> O    \n",
            "the                   -->  O     --> O    \n",
            "U.N.                  -->  B-org --> B-org\n",
            "Security              -->  I-org --> I-org\n",
            "Council               -->  I-org --> I-org\n",
            "for                   -->  O     --> O    \n",
            "possible              -->  O     --> O    \n",
            "sanctions             -->  O     --> O    \n",
            ".\n",
            "                    -->  O     --> O    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3092/57801177.py:154: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)\n",
            "  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"
          ]
        }
      ],
      "source": [
        "i = random.choices(list(range(len(test_sents))))[0]\n",
        "text = test_sents[i]\n",
        "true_label = test_tags[i].strip().split(\" \")\n",
        "text_tensor, lengths = process_data(text)\n",
        "print(text_tensor.shape, lengths.shape)\n",
        "\n",
        "logits = model(text_tensor, lengths)\n",
        "mask = torch.ones_like(text_tensor)\n",
        "\n",
        "model.crf.gpu = False\n",
        "_, preds = model.crf._viterbi_decode(logits, mask)\n",
        "\n",
        "preds = preds.numpy()[0]\n",
        "pred_labels = [idx2tag[p] for p in preds]\n",
        "\n",
        "for w, p, t in zip(text.split(\" \"), pred_labels, true_label):\n",
        "    print(f\"{w:<20}  -->  {p:<5} --> {t:<5}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lighting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}